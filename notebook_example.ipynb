{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Benthic Megafauna using a CNN\n",
    "\n",
    "Before you can use this notebook, make sure that you have downloaded the basml.py (python) file and put it into your working directory. Also, make sure that you have all of the required libraries installed; libraries can be installed using:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can git clone the whole repository https://github.com/brett-hosking/BASML_DataChallenge.git\n",
    "\n",
    "See https://github.com/brett-hosking/BASML_DataChallenge for more details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and extract the data: \n",
    "'https://github.com/brett-hosking/BASML_DataChallenge/blob/master/data256.zip?raw=true'\n",
    "\n",
    "Download basml.py file:\n",
    "'https://github.com/brett-hosking/BASML_DataChallenge/blob/master/basml.py?raw=true'\n",
    "\n",
    "Download requirements.txt file:\n",
    "'https://github.com/brett-hosking/BASML_DataChallenge/blob/master/requirements.txt?raw=true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten\n",
    "import basml # some pre-made functions for this data challenge\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data using pre-made function in the basml mini library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data: (2094, 256, 256, 3)\n",
      "labels: (2094, 4)\n"
     ]
    }
   ],
   "source": [
    "X,Y,classlist = basml.loaddata('data256',labelstr=True) # the folder containing the downloaded data for this challenege\n",
    "print('data:', np.shape(X))\n",
    "print('labels:', np.shape(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cnidaria', 'amperima', 'tunicate', 'polychaete']\n"
     ]
    }
   ],
   "source": [
    "print(classlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normalise the values, in range [0,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X /= 255.0 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random training and test. Here we use 80% of the data for training and 20% for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = basml.randomiseXY(X,Y)\n",
    "Xtrain,Ytrain,Xtest,Ytest = basml.ttsplit(X,Y,per=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1675, 256, 256, 3) (1675, 4) (419, 256, 256, 3) (419, 4)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(Xtrain),np.shape(Ytrain), np.shape(Xtest), np.shape(Ytest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply zeromean normalisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeromean = np.mean(Xtrain)\n",
    "Xtrain -=zeromean\n",
    "Xtest -=zeromean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/wilski/venv/basml/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential()\n",
    "\n",
    "#add model layers\n",
    "model.add(Conv2D(64, kernel_size=3, activation='relu', input_shape=(256,256,3)))\n",
    "model.add(Conv2D(32, kernel_size=3, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(4, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the CNN using the training data (using the validation_data hyperparameter is optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1675 samples, validate on 419 samples\n",
      "WARNING:tensorflow:From /Users/wilski/venv/basml/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/5\n",
      "1675/1675 [==============================] - 266s 159ms/sample - loss: 1.3573 - acc: 0.6275 - val_loss: 0.7510 - val_acc: 0.7303\n",
      "Epoch 2/5\n",
      "1675/1675 [==============================] - 251s 150ms/sample - loss: 0.5861 - acc: 0.8119 - val_loss: 0.6226 - val_acc: 0.7852\n",
      "Epoch 3/5\n",
      "1675/1675 [==============================] - 245s 146ms/sample - loss: 0.4147 - acc: 0.8704 - val_loss: 0.5151 - val_acc: 0.8568\n",
      "Epoch 4/5\n",
      "1675/1675 [==============================] - 296s 177ms/sample - loss: 0.4288 - acc: 0.8860 - val_loss: 0.6741 - val_acc: 0.8616\n",
      "Epoch 5/5\n",
      "1675/1675 [==============================] - 392s 234ms/sample - loss: 0.2750 - acc: 0.9272 - val_loss: 0.4307 - val_acc: 0.8568\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f184d940>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(Xtrain, Ytrain, epochs=5,validation_data=(Xtest, Ytest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the model to predict the class of samples in the test set and reformat as one-hot, e.g. [0,1,0,0] for the second class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlen = len(Ytest)\n",
    "Ypred= np.array(np.zeros((testlen,4)))\n",
    "predictions = model.predict(Xtest)\n",
    "for i in range(testlen):\n",
    "    Ypred[i][np.argmax(predictions[i])] \t= 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use scikit-learn to generate a report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    cnidaria      0.899     0.970     0.933       101\n",
      "    amperima      0.876     0.885     0.880       104\n",
      "    tunicate      0.790     0.867     0.827       113\n",
      "  polychaete      0.877     0.703     0.780       101\n",
      "\n",
      "   micro avg      0.857     0.857     0.857       419\n",
      "   macro avg      0.861     0.856     0.855       419\n",
      "weighted avg      0.859     0.857     0.855       419\n",
      " samples avg      0.857     0.857     0.857       419\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = classification_report(Ytest, Ypred, target_names=classlist,digits=3,labels=range(len(classlist)))\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.8568019093078759\n"
     ]
    }
   ],
   "source": [
    "print('accuracy: ',(np.sum(np.argmax(Ytest,axis=1) == np.argmax(Ypred,axis=1))/ len(Ytest)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
